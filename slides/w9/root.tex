\documentclass{beamer}
\usetheme{Berlin}
\usecolortheme{beaver}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{amsmath}
\usepackage{lmodern}% http://ctan.org/pkg/lm
\usepackage{mathtools}
\usefonttheme{professionalfonts}

\title{Chapters 9.3-9}
\subtitle{}
\author[Riccardo \and Eren]{Riccardo~Miccini\inst{1} \and Eren~Can~\inst{1}}
\institute[DTU]
{
	\inst{1}
	Technical University of Denmark\\
	Digital Communication
}
\date{\today}
\subject{Digital Communication}

\tikzstyle{int}=[draw, fill=blue!20]
\tikzstyle{every node}=[font=\tiny]

\begin{document}
\frame{\titlepage}

% ch 9.3
\begin{frame}
	\frametitle{Modulation Schemes not Requiring Coherent References}
	\begin{itemize}
		\item In this section, now we consider two modulation schemes that you do not need to require the acquisition of  a local reference signal in phase coherence with the received carrier.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Differential Phase-Shift Keying  (DPSK)}
	\begin{itemize}
		\item The implementation of a such a scheme presupposes two things;
		\begin{enumerate}
		\item The unknown phase perturbation on the signal varies  slowly that the phase is constant from one signalling interval to next.
		\item  The phase during a given signalling interval bears a known relationship to the phase during the preceding signalling interval bears a known relationship to the phase during the preceding signalling interval.
	\end{enumerate}
	\end{itemize}
	\begin{figure}
	\includegraphics[width=0.8\textwidth]{9_3.png}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Differential Encoding Message Sequence}
	\begin{itemize}
		\item An arbitrary reference binary digit is being selected as an initial digit of the sequence
		\item For each digit , the present digit used as a reference
		\item 0 in the message sequence is encoded as  a transition from state of reference digit to the opposite state in the encoded message sequence
		\item 1 encoded as no change of state
	\end{itemize}
	\begin{figure}
		\includegraphics[width=0.8\textwidth]{9_3_1.png} \\
	\end{figure}
\end{frame}

\begin{frame}
\frametitle{Figure for Differential Encoding Message Sequence}
	\begin{figure}
		\includegraphics[width=\textwidth]{9_4_1.png}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Differential Encoding Message Sequence}
	\begin{itemize}
		\item After the reference bit and plus the first encoded bit, signal input become $S_1=A \cos(\omega_c) t$ and $R_1=A*cos(w_c)*t$
		\item Than the output correlator is; $v_1= \int_{0}^{T} A^2 \cos^2(\omega_c t) dt$ which eventually become $\frac{1}{2} A^2 T$
		\item The optimum detector for binary will become  $l= x_k x_k-1 +y_k y_k-1$
		\item Without a loss of of generality, we can choose $\theta=0$; we found outputs at $t=0$ to be:
			\begin{itemize}
			\item $x_0= \frac{A T}{2}+n_1$ and $y_0=n_3$ and where $n_1=  \int_{-T}^{0} n(t) \cos^2(\omega_c t) dt$
			\item  $n_3=\int_{-T}^{0} n(t) \sin^2(\omega_c t) dt$. Similarly, at the time $t=T$, the outputs are ;
			$x_1=\frac{AT}{2}+n_2$ and $y_1=n_4$
			\item $n_2=\int_{0}^{T} n(t) \cos^2(\omega_c t) dt$
			\item $n_4=\int_{0}^{T} n(t) \sin^2(\omega_c t) dt$
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Important Figure for Differential Encoding }
	\begin{figure}
		\includegraphics[width=0.8\textwidth]{9_4_2.png}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item It follows as $n_1, n_2, n_3 and n_4$ are uncorrelated and zero-mean Gaussian random variables with variances $\frac{N_0 T}{4}$ and they are independent.
		\item Expression for Probability error $P_E= Pr [(\frac{A T}{2}+n_1)(\frac{A T}{2}+n_2)+n_3 n_4<0]$
		\item We can define new gaussian random variables  such as:
		$ \omega_1=\frac{n_1}{2}+\frac{n_2}{2} $ \\
		$ \omega_2=\frac{n_1}{2}-\frac{n_2}{2}  $\\
		$ \omega_3=\frac{n_3}{2}+\frac{n_4}{2}$ \\
		$ \omega_4=\frac{n_3}{2}-\frac{n_4}{2} $
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item Probability can be written in terms of Gaussian variables:
		$P_E= Pr [(\frac{A T}{2}+\omega_1)^2 +(\omega_3)^2<(\omega_2^2+\omega_4^2)]$
		\item Gaussian variables will also let us define the Ricean random variables. Ricean random variable will become: $R_1=\sqrt{(\frac{AT}{2}+\omega_1)^2+\omega_3^2}$
		\item Also Rayleigh random variable will become $R_2=\sqrt{\frac{\omega_2^2}{\omega_4^2}}$
		\item If we also define the bit energy $E_b$ as $A^2 \frac{A^2 T}{2}$ will give;
		$P_E=\frac{1}{2} e^(\frac{-E_b}{N_0}$ for the optimum DPSK receiver.
		\item At the large values  $\frac{-E_b}{N_0}$ values of ;
		$P_E=Q[\sqrt{\frac{-E_b}{N_0}}]=Q[\sqrt{z}]$
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
	\item Following result obtained by using the asymptotic approximation;
	$P_E=\frac{e^(-E_b/N_0)}{2 \sqrt{\pi \frac {E_b}{N_0}}}$
	\end{itemize}
\begin{figure}
	\includegraphics[width=0.8\textwidth]{9_5.png}
\end{figure}
\end{frame}

% ch 9.5
\begin{frame}
	\frametitle{Comparison of Digital Modulation Systems}
	\begin{itemize}
		\item Bit error probabilities are compared in Figure 9.22 for the modulation schemes that considered in this chapter. Note that the curve for antipodal binary PAM is identical to BPSK
		\item  Also bit error probability of antipodal PAM  becomes worse the larger M. Curves move more to the right as M gets larger
	\end{itemize}
\end{frame}

	\begin{frame}
	\frametitle{Important figure for Chapter 9}
	\begin{figure}
\includegraphics[width=0.5\textwidth]{9_5_1.png}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{itemize}
	\item Non-coherent binary FSK and PAM with M=4 have almost identical performance at large signal-to-noise ratios.
	\item	In addition to cost and complexity implementation, there are many other considerations in choosing one type of digital data system over another.
	\item Some channels, where the channel gain, phase or when both are in effect,we use a noncoherent system may be dictated because of impossibility of establishing a coherent reference at the receiver  under such conditions. They will be referred as "fading".
	\end{itemize}
\end{frame}


% ch 9.7
\begin{frame}
	\frametitle{Multipath Interference (1)}
	\begin{itemize}
		\item Additive Gaussian noise is not sufficient to accurately model the transmission channel
		\item Other sources of degradation:
		\begin{itemize}
			\item bandwidth limiting by the channel
			\item impulse noise (lightnings, switching)
			\item RF interference from other transmitters
			\item \emph{multipath interference} from signal reflections and scattering
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Multipath Interference (2)}
	\begin{itemize}
		\item Two-way multipath model: $ y(t) = s_d(t) + \beta s_d(t - \tau_m) + n(t) $
		\begin{description}
			\item[$ n(t) $] Gaussian noise component
			\item[$ s_d(t) $] Signal from the direct path
			\item[$ \beta $] Gain of secondary path component
			\item[$ \tau_m $] Time delay of secondary path component
		\end{description}
		\item For binary phase-shift keying signals: $ s_d(t) - Ad(t)\cos(\omega_c t) $
		\begin{description}
			\item[$ d(t) $] Data stream (sequence of $ \pm1 $ rectangular pulses) of width T
			\item[$ \omega_c $] Carrier frequency
		\end{description}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Multipath Interference (3)}
	\begin{itemize}
		\item Input of the integrator at the receiving end: $ x(t) = LP\{2y(t)\cos(\omega_c t)\} = Ad(t) + \beta Ad(t - \tau_m)\cos(\omega_c \tau_m) + n_c(t) $
		\item Two scenarios:
		\begin{description}
			\item[$ \tau_m/T \cong 0 $] The original and reflected signals are almost congruent, so $ \omega_c \tau_m $ is uniformly distributed in $ [-\pi, \pi] $. When many reflection components are considered, the envelope of the signal assumes a Rayleigh or Ricean distribution
			\item[$ 0 < \tau_m/T \leq 1 $] Adjacent bits in the original and reflected signals overlap; inter-symbol interference appears
		\end{description}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Multipath Interference - second scenario (1)}
	\begin{itemize}
		\item Four equally likely cases; total probability of error is: $ P_E = \frac{1}{4}[P(E|++) + P(E|-+) + P(E|+-) + P(E|--)] $
		\item Noise on the integrator integrator out is Gaussian-distributed with $ \mu = 0 $ and $ \omega^2_n = N_0T $
		\item Due to the symmetric nature of the overlapping bits and Gaussian probability density function, only two cases need to be computed
		\begin{itemize}
			\item $ P(E|++) = P(E|--) = Q\left[\sqrt{\frac{2E_b}{N_0}}(1+\delta)\right] $
			\item $ P(E|+-) = P(E|-+) = Q\left[\sqrt{\frac{2E_b}{N_0}}\left((1+\delta) - \frac{2\delta\tau_m}{T}\right)\right] $
		\end{itemize}
		\item After substituting the cases above into the matching ones: $ P_E = \frac{1}{2}Q\left[\sqrt{2z_0}(1+\delta)\right] + \frac{1}{2}Q\left[\sqrt{2z_0}\left((1+\delta) - \frac{2\delta\tau_m}{T}\right)\right] $
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Multipath Interference - second scenario (2)}
	\begin{itemize}
		\item Overal probability of error changes with $ z_0 = \frac{E_b}{N_0} = \frac{A^2T}{2N_0} $
	\end{itemize}
	\begin{figure}
		\includegraphics[width=\textwidth, height=0.7\textheight, keepaspectratio]{pe_vs_z0.png}
	\end{figure}
\end{frame}


% ch 9.9
\begin{frame}
	\frametitle{Equalization}
	\begin{itemize}
		\item Equalization is used in telecommunication to reverse the signal degradation caused by multipath propagation and bandwidth limitations
		\item Simplest form of equalization consists in an inverse filter - \emph{Tapped-delay-line filter}
		\item Two ways of determining the filter coefficients:
		\begin{itemize}
			\item zero-forcing
			\item mean-square error
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Equalization by Zero Forcing}
	\begin{itemize}
		\item Impulse response of equalized output: $ p_{eq}(mT) = \sum_{n=-N}^N {\alpha_n p_c((m-n)T)} \Rightarrow [P_{eq}]=[P_c][A] $
		\item $ [P_{eq}] $ is a column vector composed of: N zeros, 1, N zeros
		\item Equalizarion filter coefficient matrix: $ [A]_{opt} = [P_c]^{-1}[P_{eq}] $
		\item Multiplying by $ [P_{eq}] $ corresponds to picking the middle column of matrix $ [P_c]^{-1} $
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Equalization by Minimum Mean-Squared Error (1)}
	\begin{itemize}
		\item Obtain filter coefficients that minimize the difference between the output of the equalizer and the actual output: $ \varepsilon = E\left\{[z(t) - d(t)]^2\right\} = minimum $
		\begin{itemize}
			\item $ z(t) $ is the equalizer output response (incl. noise): $ z(t) = \sum_{n=-N}^N {\alpha_n p_c((m-n)T)} $
			\item $ d(t) $ is the desired response
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Equalization by Minimum Mean-Squared Error (2)}
	\begin{itemize}
		\item $ \varepsilon $ is concave and can be minimized by derivation: $ \frac{\delta\varepsilon}{\delta\alpha_m} = 0 = 2E\left\{[z(t) - d(t)] \frac{\delta z(t)}{\delta\alpha_m} \right\} $
		\item Substituting $ z(t) $ gives the following conditions (in terms of cross-correlation): $ R_{yz}(m\Delta) = R_{yd}(m\Delta) = 0 $
		\begin{itemize}
			\item $ R_{yz}(\tau) = E[y(t)z(t + \tau)] $
			\item $ R_{yd}(\tau) = E[y(t)d(t + \tau)] $
		\end{itemize}
		\item In terms of matrices: $ [R_{yy}][A]_{opt} = [R_{yd}] $
		\item Solving for the filter taps: $ [A]_{opt} = [R_{yy}]^{-1}[R_{yd}] $
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Tap Weight Ajustment (LMS Algorithm) (1)}
	\begin{itemize}
		\item How to obtain $ d(t) $
		\begin{enumerate}
			\item Periodically send known data sequence used for weight adjustment
			\item Use method 1 for first guess and then use detected data (\emph{decision-directed} mode)
		\end{enumerate}
		\item Apply gradient descent to initial weight values ($ [A]^{(0)} $): $ [A]^{(k+1)} = [A]^{(k)} + \frac{1}{2}\mu [-\nabla\varepsilon^{(k)}] $
		\begin{description}
			\item[$ k $] iteration of weights calculation
			\item[$ \nabla\varepsilon $] slope of error surface
			\item[$ \mu $] size of the step
		\end{description}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Tap Weight Ajustment (LMS Algorithm) (2)}
	\begin{itemize}
		\item Alternative approach (Least-Mean-Square): $ \alpha^{(k+1)}_m = \alpha^{(k)}_m - \mu y[(k - m)\Delta] \epsilon(k\Delta) $
		\item $ \epsilon(k\Delta) $ is the error given by $ y_{eq}(k\Delta) - d(k\Delta) $
		\begin{description}
			\item[$ y_{eq}(k\Delta) $] equalization filter output
			\item[$ d(k\Delta) $] data sequence used for training
		\end{description}
	\end{itemize}
\end{frame}

\end{document}
